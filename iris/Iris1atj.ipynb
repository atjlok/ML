{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f957ca01",
   "metadata": {},
   "source": [
    "## Your \"First\" Step-By-Step -  Machine Learning Project in Python .\n",
    "\n",
    "\n",
    "## üìåExpert Insight : \n",
    "\n",
    "\n",
    "**You Can Do Machine Learning in Python :**\n",
    "\n",
    "- Work through this tutorial below. It will take you 5-to-10 minutes, max!\n",
    "<br>\n",
    "\n",
    "- **You do not need to be a Python programmer.** The syntax of the Python language can be intuitive if you are new to it. Just like other languages, focus on function calls (e.g. function()) and assignments (e.g. a = ‚Äúb‚Äù). This will get you most of the way. You are a developer, you know how to pick up the basics of a language real fast. Just get started and dive into the details later.\n",
    "<br>\n",
    "- **You do not need to understand everything.** (at least not right now) Your goal is to run through the tutorial end-to-end and get a result. You do not need to understand everything on the first pass. List down your questions as you go. Make heavy use of the help(‚ÄúFunctionName‚Äù) help syntax in Python to learn about all of the functions that you‚Äôre using.\n",
    "<br>\n",
    "- **You do not need to know how the algorithms work.** It is important to know about the limitations and how to configure machine learning algorithms. But learning about algorithms can come later. You need to build up this algorithm knowledge slowly over a long period of time. Today, start off by getting comfortable with the platform.\n",
    "<br>\n",
    "- **You do not need to be a machine learning expert.** You can learn about the benefits and limitations of various algorithms later.\n",
    "<br>\n",
    "\n",
    "- **Consider this as your first project-** Focus on the key steps., namely, loading data, looking at the data, evaluating some algorithms and making some predictions. In later tutorials we will focus on other data preparation and result improvement tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03644050",
   "metadata": {},
   "source": [
    "##  üìå Let's go... Step-By-Step... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fd996",
   "metadata": {},
   "source": [
    "\n",
    "### üìç A machine learning project may not be linear, but it has a number of well known steps\n",
    "\n",
    "   - 1. Define the Problem.\n",
    "   - 2. Prepare the Data.\n",
    "   - 3. Evaluate the Algorithms.\n",
    "   - 4. Improve the Results.\n",
    "   - 5. Present the Results.\n",
    "   \n",
    "   \n",
    "## 1. Required Libraries\n",
    "\n",
    "### üìç There are 5 key libraries that you will need.\n",
    "\n",
    "- numpy\n",
    "- pandas\n",
    "- scipy \n",
    "- sklearn\n",
    "- matplotlib \n",
    "\n",
    "\n",
    "## üìç 1.1 Start Python and Check Versions\n",
    "\n",
    "- It is always advised to make sure your Python environment was installed successfully and is working as expected.\n",
    "\n",
    "- The script below will help you understand your environment. \n",
    "\n",
    "- It imports each library required for this tutorial and prints the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the versions of libraries\n",
    " \n",
    "# Python version\n",
    "import sys\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fe09d",
   "metadata": {},
   "source": [
    "## üìç 2. Load The Data\n",
    "\n",
    "- The iris flowers dataset. \n",
    "\n",
    "- This dataset is famous because it is used as the ‚Äúhello world‚Äù dataset in machine learning and statistics.\n",
    "\n",
    "- The dataset contains 150 observations of iris flowers. \n",
    "\n",
    "- There are four columns of measurements of the flowers in centimeters. \n",
    "\n",
    "- The fifth column is the species of the flower observed. \n",
    "\n",
    "- All observed flowers belong to one of three species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acf3b5",
   "metadata": {},
   "source": [
    "## üìç 2.1 Import libraries\n",
    "\n",
    "- First, import all of the modules, functions and objects needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a4220",
   "metadata": {},
   "source": [
    "## üìç 2.2 Load Dataset\n",
    "\n",
    "- We can load the data directly from the UCI Machine Learning repository.\n",
    "\n",
    "- We are using pandas to load the data. \n",
    "\n",
    "- We will also use pandas next to explore the data both with descriptive statistics and data visualization.\n",
    "\n",
    "#### Note that we are specifying the names of each column when loading the data. This will help later when we explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "a = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acb459",
   "metadata": {},
   "source": [
    "**The dataset should load without incident.**\n",
    "\n",
    "If you do have network problems, you can download the iris.csv file into your working directory and load it using the same method, changing URL to the local file name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fa874",
   "metadata": {},
   "source": [
    "## üìç 3. Summarize the Dataset\n",
    "\n",
    "**Now it is time to take a look at the data.**\n",
    "\n",
    "In this step we are going to take a look at the data in a few different ways:\n",
    "\n",
    "- Dimensions of the dataset.\n",
    "- Peek at the data itself.\n",
    "- Statistical summary of all attributes.\n",
    "- Breakdown of the data by the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443890a4",
   "metadata": {},
   "source": [
    "## üìç 3.1 Dimensions of Dataset\n",
    "\n",
    "We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2564af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "a = df\n",
    "\n",
    "b = a.shape()  # You should see 150 instances and 5 attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02a27a",
   "metadata": {},
   "source": [
    "## üìç 3.2 Peek at the Data\n",
    "\n",
    "It is always advisable to actually eyeball your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7681273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "a = df\n",
    "\n",
    "c = a.head(20)  # You should see the first 20 rows of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c5554",
   "metadata": {},
   "source": [
    "## üìç 3.3 Statistical Summary\n",
    "\n",
    "Now, let us take a look at a summary of each attribute.\n",
    "\n",
    "This includes the count, mean, the min and max values as well as some percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "a = df\n",
    "\n",
    "# descriptions\n",
    "\n",
    "d = a.describe().transpose() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a90a8",
   "metadata": {},
   "source": [
    "## üìç 3.4 Class Distribution\n",
    "\n",
    "Now, let us take a look at the number of instances (rows) that belong to each class. \n",
    "\n",
    "We can view this as an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc23c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "a = df\n",
    "\n",
    "# class distribution\n",
    "c = a.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69372f3d",
   "metadata": {},
   "source": [
    "#### We can see that each class had the same number of instances (50 or 33% of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28f3a2",
   "metadata": {},
   "source": [
    "## üìç 4. Data Visualization\n",
    "\n",
    "- As We now have a basic idea about the data. We can extend our understanding with some visualizations.\n",
    "\n",
    "**We are going to use two types of plots:**\n",
    "\n",
    "- Univariate plots to better understand each attribute.\n",
    "- Multivariate plots to better understand the relationships between attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83deb0e5",
   "metadata": {},
   "source": [
    "## üìç 4.1 Univariate Plots\n",
    "\n",
    "- these are plots of each individual variable.\n",
    "\n",
    "- As the input variables are numeric, we can create box and whisker plots of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "# plot the data\n",
    "\n",
    "plt.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6230685",
   "metadata": {},
   "source": [
    "### This output gives us a much clear idea of the distribution of the input attributes:\n",
    "\n",
    "**We can also create a histogram of each input variable to get an idea of the distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "# Create histograms for one of the features (e.g., 'sepal-length')\n",
    "\n",
    "plt.hist(df['sepal-length'], bins=10, color='blue', edgecolor='black')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Sepal Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d728550",
   "metadata": {},
   "source": [
    "**It looks like two of the input variables have a Gaussian distribution.**\n",
    "\n",
    "This will be useful as we can use algorithms that can exploit this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f67ff",
   "metadata": {},
   "source": [
    "## üìç 4.2 Multivariate Plots\n",
    "\n",
    "- Used to look at the interactions between the variables.\n",
    "\n",
    "- First, let us look at scatterplots of all pairs of attributes. This will be helpful to spot structured relationships between input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e56e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "# scatter plot matrix\n",
    "\n",
    "scatter_matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec502382",
   "metadata": {},
   "source": [
    "**Note: the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0852f0",
   "metadata": {},
   "source": [
    "## üìç 5. Evaluate Some Algorithms\n",
    "\n",
    "**Now it is time to create some models of the data and estimate their accuracy on unseen data.**\n",
    "\n",
    "**This is what we are going to do in this step:**\n",
    "\n",
    "- Separate out a validation dataset.\n",
    "- Set-up the test harness to use 10-fold cross validation.\n",
    "- Build multiple different models to predict species from flower measurements\n",
    "- Select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acfa6c",
   "metadata": {},
   "source": [
    "## üìç 5.1 Create a Validation Dataset\n",
    "\n",
    "- First , we need to know whether the model we created is good.\n",
    "\n",
    "- Further down , we will use statistical methods to estimate the accuracy of the models that we create on unseen data. \n",
    "\n",
    "- We also need a more concrete estimate of the accuracy of the best model on unseen data by evaluating it on actual unseen data.\n",
    "\n",
    "- We are going to hold back some data that the algorithms will not get to see and we will use this data to get a second and independent idea of how accurate the best model might actually be.\n",
    "\n",
    "**The method :**\n",
    "\n",
    "- We will split the loaded dataset into two,\n",
    "\n",
    "   - 80% of which we will use to train, evaluate and select among our models, \n",
    "   \n",
    "   - 20% that we will hold back as a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "df = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "# Split-out validation dataset\n",
    "\n",
    "array = df.values\n",
    "X = array[:,0:4]\n",
    "y = array[:,4]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081160e",
   "metadata": {},
   "source": [
    "**Now we have training data in the X_train and Y_train for preparing models and a X_validation and Y_validation sets that we can use later.**\n",
    "\n",
    "Notice that we used a python slice to select the columns in the NumPy array. \n",
    "\n",
    "- If this is new to you, you can refer our numpy - practice. ( How to Index, Slice and Reshape NumPy Arrays.)  - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a42949",
   "metadata": {},
   "source": [
    "## üìç 5.2 Test Harness\n",
    "\n",
    "We will use stratified 10-fold cross validation to estimate model accuracy.\n",
    "\n",
    "This will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.\n",
    "\n",
    "**Stratified** means that each fold or split of the dataset will aim to have the same distribution of example by class as it exists in the whole training dataset.\n",
    "\n",
    "- We set the random seed via the random_state argument to a fixed number \n",
    "  - to ensure that each algorithm is evaluated on the same splits of the training dataset.\n",
    "- The specific random seed does not matter, \n",
    "  - we are using the metric of ‚Äòaccuracy‚Äò to evaluate models.\n",
    "- This is a ratio of the number of correctly predicted instances divided by the total number of instances in the dataset    \n",
    "  - which is multiplied by 100 to give a percentage (e.g. 95% accurate). \n",
    "\n",
    "We will be using the scoring variable when we run build and evaluate each model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616a0ee",
   "metadata": {},
   "source": [
    "## üìç 5.3 Build Models\n",
    "\n",
    "We do not know which algorithms would be good on this problem or what configurations we need to use.\n",
    "\n",
    "We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we can generally expect good results.\n",
    "\n",
    "**Let us test 6 different algorithms:**\n",
    "\n",
    "- Logistic Regression (LR)\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- K-Nearest Neighbors (KNN).\n",
    "- Classification and Regression Trees (CART).\n",
    "- Gaussian Naive Bayes (NB).\n",
    "- Support Vector Machines (SVM).\n",
    "\n",
    "**This is a good combination of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms.**\n",
    "\n",
    "Let us build and evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ecb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Spot Check Algorithms\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23c1a8",
   "metadata": {},
   "source": [
    "## üìç 5.4 Select Best Model\n",
    "\n",
    "- We now have 6 models and accuracy estimations for each. \n",
    "- We need to compare the models with each other and select the most accurate.\n",
    "\n",
    "**Running the example above, we got a few raw results:**\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. \n",
    "\n",
    "- Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "- In this case, we can see that it looks like Support Vector Machines (SVM) has the largest estimated accuracy score.\n",
    "\n",
    "- We can also create a plot of the model evaluation results and compare the spread and the mean accuracy of each model. \n",
    "\n",
    "- There is a population of accuracy measures for each algorithm because each algorithm was evaluated 10 times (via 10 fold-cross validation).\n",
    "\n",
    "**One best way to compare the samples of results for each algorithm is to create a box and whisker plot-**\n",
    "    - for each distribution and compare the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ef3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4dd52",
   "metadata": {},
   "source": [
    "**We can see that the box and whisker plots are squashed at the top of the range, with many evaluations achieving 100% accuracy, and some pushing down into the high 80% accuracies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34d5b6",
   "metadata": {},
   "source": [
    "## üìç 5.5 Complete Example\n",
    "\n",
    "- For reference, we will group all of the previous code-blocks together into a single script.\n",
    "\n",
    "- The combined code-block is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "\n",
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:4]\n",
    "y = array[:,4]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    \n",
    "# Compare Algorithms\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab5ae5",
   "metadata": {},
   "source": [
    "## üìç 6. Make Predictions\n",
    "\n",
    "**We need to choose an algorithm to use to make predictions.**\n",
    "\n",
    "- The results in the previous section suggest that the SVM was perhaps the most accurate model. \n",
    "\n",
    "- We will use SVM as our final model.\n",
    "\n",
    "Now we need an idea of the accuracy of the model on our validation set.\n",
    "\n",
    "- This will give us an independent final check on the accuracy of the best model. \n",
    "\n",
    "It is always valuable to keep a validation set to avoid slips made during the training process, you may encounter overfitting to the training set or a data leak. Both of these issues will make your result overly optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f9423",
   "metadata": {},
   "source": [
    "## üìç 6.1 Make Predictions\n",
    "\n",
    "**We can fit the model on the entire training dataset and make predictions on the validation dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation dataset\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645c639",
   "metadata": {},
   "source": [
    "We might also like to make predictions for single rows of data. For examples on how to do that, see the tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a70dd",
   "metadata": {},
   "source": [
    "## üìç 6.2 Evaluate Predictions\n",
    "\n",
    "We can evaluate the predictions by comparing them to the expected results in the validation set,\n",
    "- then calculate classification accuracy, as well as a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba58290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1e7ff",
   "metadata": {},
   "source": [
    "We can see that the accuracy is 0.966 or about 96% on the hold out dataset.\n",
    "\n",
    "The confusion matrix provides an indication of the errors made.\n",
    "\n",
    "Lastly, the classification report provides a breakdown of each class by precision, recall, f1-score and support showing excellent results (considering the validation dataset was small)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dff907",
   "metadata": {},
   "source": [
    "## üìç 6.3 Complete Example\n",
    "\n",
    "- For reference, we will group all of the previous code-blocks together into a single script.\n",
    "\n",
    "- The combined code-block is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70bfd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv('iris.csv', names=names)\n",
    "\n",
    "\n",
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:4]\n",
    "y = array[:,4]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
